# AI 产品 — CPO 评审清单

> 类型: ai-product
> 适用: 以 AI/ML 能力为核心价值的产品，包括 AI SaaS 工具、AI Agent、AI 辅助决策系统等。本清单特别关注 AI 产品独有的设计挑战：不确定性管理、用户信任建设、成本控制。
> 版本: v1.0

---

## 核心考量维度

### 用户路径

AI 产品的用户路径核心特征是**信任渐进**——用户需要逐步建立对 AI 能力的信任：

```
理解 AI 能力 → 试用 Demo → 集成/使用 → 看到结果 → 建立信任 → 扩大使用
```

**关键路径说明：**

| 触点 | 用户行为 | 用户心理 | 设计要点 |
|------|---------|---------|---------|
| **理解 AI 能力** | 阅读产品介绍，理解 AI 能做什么 | "AI 能做到吗？会不会是忽悠？" | 用具体案例而非抽象描述；展示 before/after 对比；明确能力边界（能做什么+不能做什么） |
| **试用 Demo** | 在 Playground 或 Demo 中体验 | "让我自己试试看" | 零门槛体验 → 必须有 wow moment；Demo 用真实场景而非 toy example |
| **集成/使用** | 将 AI 能力接入自己的工作流 | "能用在我的实际场景吗？" | 提供清晰的集成文档/SDK；快速启动模板；首次使用引导 |
| **看到结果** | AI 产出第一个有价值的结果 | "结果靠谱吗？比我手动做好多少？" | 结果可视化 + 质量分数；与人工结果对比；允许修改和反馈 |
| **建立信任** | 多次使用后确认 AI 的稳定性 | "它大部分时候靠谱吗？" | 错误时优雅降级；持续的质量反馈机制；透明的准确率数据 |
| **扩大使用** | 覆盖更多场景，购买更高套餐 | "还能用在哪？值得为此付更多吗？" | 发现新用例的引导；用量仪表盘 + ROI 计算器 |

**分支路径：**
- **API 集成路径**：文档 → API Key → 测试调用 → 生产集成（开发者优先的产品）
- **Agent 代理路径**：配置 Agent → 分配任务 → 监督执行 → 评估结果（AI Agent 类产品）
- **嵌入式路径**：在现有工具中使用 AI 功能 → 自然发现 → 依赖形成（如 Copilot 模式）

---

### 关键转化节点

| 转化节点 | 目标行为 | 行业基准 CVR | 关键影响因素 |
|---------|---------|-------------|-------------|
| **Demo 体验** | 用户完成一次 AI 交互并对结果满意 | 30-50% (从 Demo 到注册) | Demo 的 wow moment 质量、是否用用户自己的数据、结果可信度 |
| **首次真实结果** | 在自己的场景中得到有价值的 AI 输出 | 40-60% (注册到首次价值体验) | Time-to-first-value (TTFV) 长度、结果质量、错误处理 |
| **习惯形成** | 连续使用 ≥ 3 次 | 25-40% (从首次使用到习惯) | AI 结果稳定性、人工干预频率、效率提升感知 |
| **付费转化** | 免费用户转付费 | 10-20% (AI SaaS) | 免费额度设计、付费功能差异化、使用量触顶提醒 |

**转化漏斗优化优先级：**
1. **Demo 体验** — AI 产品 Demo 是最关键的信任节点，必须有 wow moment
2. **首次真实结果** — TTFV（Time to First Value）决定留存，目标 < 5 分钟
3. **习惯形成** — AI 产品的 Day 7 留存比 Day 1 更重要，关注连续使用

---

### 必备功能清单

#### P0 — 没有不行

| # | 功能 | 解决什么问题 | 验收标准 |
|---|------|------------|---------|
| 1 | **AI 能力的清晰说明** | 用户需要准确理解 AI 能做什么、不能做什么 | 用"能做/不能做"对比说明；有真实 case 的 before/after；不使用"万能""智能"等模糊词 |
| 2 | **交互式 Demo/Playground** | 用户必须亲自体验才能建立信任 | 无需注册即可体验核心 AI 能力；Demo 使用真实场景数据（不是 "Hello World"）；结果在 10 秒内返回 |
| 3 | **结果质量保障** | AI 结果不 100% 准确，用户需要质量信心 | 每个 AI 输出附带置信度/质量分数；支持用户对结果反馈（好/不好）；低置信度结果有明确标注 |
| 4 | **AI 能力透明化** | 用户需要理解 AI 是怎么工作的（至少大概） | 对结果有简要的解释说明；不是完全黑箱；用户可以理解为什么得到这个结果 |
| 5 | **AI 失败时的 Fallback** | AI 不可能 100% 成功，失败时不能让用户无路可走 | AI 失败时有明确的错误提示；提供人工替代方案或手动完成路径；不会因为 AI 失败导致整个流程卡住 |

#### P1 — 强烈建议

| # | 功能 | 价值说明 | 建议加入版本 |
|---|------|---------|-------------|
| 1 | **Human-in-the-loop** | 让用户可以修正 AI 结果，既提升质量又收集训练数据 | V1.0 |
| 2 | **历史结果回溯** | 查看和复用之前的 AI 输出 | V1.1 |
| 3 | **自定义配置** | 让用户调整 AI 行为（如风格、详细程度、偏好） | V1.1 |
| 4 | **批量处理** | 从单个请求扩展到批量处理 | V1.2 |
| 5 | **API 接入** | 允许将 AI 能力集成到用户自己的系统中 | V1.1 |
| 6 | **使用量仪表盘** | 用户了解自己的使用量和 AI 节省的时间/成本 | V1.1 |
| 7 | **反馈闭环** | 用户反馈持续改善 AI 质量 | V1.0 （数据收集）→ V1.1（用于模型优化） |

---

### 常见陷阱

| # | 陷阱 | 典型表现 | 后果 | 预防措施 |
|---|------|---------|------|---------|
| 1 | **过度承诺 AI 准确率** | "AI 准确率 99%"但实际只有 85% | 用户期望落差导致信任崩塌 | 诚实标注准确率范围；用"成功率 85-90%"代替"99%"；展示真实数据 |
| 2 | **无 Human-in-the-loop** | AI 输出后没有编辑/修正能力 | 用户对低质量结果束手无策，只能放弃 | 每个 AI 输出都可以编辑；提供"重新生成"选项；收集修正数据用于改进 |
| 3 | **黑箱设计** | 用户完全不知道 AI 为什么给出这个结果 | 用户无法建立信任，也无法有效使用 | 提供结果解释（即使是简化的）；展示 AI 考虑的关键因素 |
| 4 | **忽略边缘情况** | 只测试了 happy path，没有考虑异常输入 | 真实用户的输入千奇百怪，AI 频繁出错 | 定义输入规范 + 输入验证；边缘情况的优雅降级；建立异常输入的测试集 |
| 5 | **AI 失败无处理** | API 超时/模型报错时只显示错误码 | 用户不知道发生了什么，也不知道该怎么办 | 友好的错误信息 + 重试按钮 + 手动替代方案 + 客服入口 |
| 6 | **成本失控** | 没有 Token/API 调用的成本意识 | 每个请求的 AI 成本 > 收入，越用越亏 | 从 Day 1 追踪每请求成本；设定成本预算；优化 Prompt 降低 Token 消耗 |
| 7 | **Demo 不够 wow** | Demo 用 toy example，不能打动用户 | 用户"看起来不过如此"，不注册 | Demo 必须用用户真实场景的数据；结果必须在 10 秒内可见 |
| 8 | **全自动妄想** | 想做全自动，不给用户控制权 | 用户不信任全自动，宁可用半自动 | 渐进式自动化：先半自动+人工监督 → 建立信任后逐步全自动 |

---

### 技术考量

#### 架构选型

| 决策点 | 选项 A | 选项 B | 推荐 | 考量因素 |
|--------|--------|--------|------|---------|
| **AI 调用模式** | 同步（等待结果） | 异步（提交后通知） | 按响应时间决定 | < 10s → 同步；> 10s → 异步 + 进度反馈 |
| **模型部署** | 调用第三方 API（OpenAI/Claude） | 自建模型服务 | 第三方 API → MVP | 自建成本高、周期长；MVP 阶段用第三方 API 快速验证 |
| **Prompt 管理** | 代码硬编码 | Prompt 管理系统（版本化 + A/B） | 管理系统 | Prompt 是 AI 产品的核心资产，必须版本化管理 |
| **结果缓存** | 不缓存 | 相似输入缓存结果 | 视场景缓存 | 相同输入的结果可以缓存以降低成本和延迟 |

#### API 设计

| 设计要点 | 说明 | 实现方式 |
|---------|------|---------|
| **Rate Limiting** | 防止滥用和成本失控 | 按用户/套餐设置 QPS 和日/月额度 |
| **超时处理** | AI 调用可能耗时较长 | 设置合理超时（30-60s）+ 客户端重试机制 |
| **流式响应** | 长文本生成时的用户体验 | Server-Sent Events (SSE) 或 WebSocket 流式输出 |
| **幂等性** | 重试不应产生重复计费/副作用 | 使用 idempotency key |
| **版本管理** | API 变更不破坏现有集成 | URL 版本化（/v1/、/v2/）+ 变更日志 |

#### Token 成本管理

| 策略 | 说明 | 预期效果 |
|------|------|---------|
| **Prompt 优化** | 精简 System Prompt，减少冗余指令 | Token 消耗降低 20-40% |
| **模型分级** | 简单任务用小模型，复杂任务用大模型 | 成本降低 50-70%（大部分请求可用小模型） |
| **结果缓存** | 相似输入复用之前的结果 | 命中率高时成本降低 30-50% |
| **输入预处理** | 清洗和压缩用户输入 | 减少无效 Token 消耗 |
| **批量合并** | 多个小请求合并为一个大请求 | 减少 overhead Token（System Prompt 摊薄） |

**成本公式：**
```
每请求成本 = (input_tokens × input_price + output_tokens × output_price) × (1 - cache_hit_rate)
每用户月成本 = 每请求成本 × 月均请求数
毛利率 = (月收入 - 月 AI 成本 - 月基础设施成本) / 月收入
```

**目标：毛利率 > 60%。如果 < 40%，需要优化成本或调整定价。**

#### 模型版本管理

| 要点 | 说明 |
|------|------|
| **Prompt 版本化** | 每次 Prompt 变更都有版本号，可以回滚 |
| **评估流水线** | 每次 Prompt/Model 变更前在评估集上跑回归测试 |
| **灰度发布** | 新版本先对 10% 用户生效，确认无退化后全量 |
| **指标对比** | 新旧版本的质量指标、延迟、成本对比 |

#### Prompt Engineering SOP

| 阶段 | 操作 | 产出 |
|------|------|------|
| **需求分析** | 明确 AI 任务的输入/输出/质量标准 | 任务定义文档 |
| **Prompt 初版** | 基于任务定义编写 Prompt | Prompt v0.1 |
| **评估集构建** | 准备 50-100 个测试用例（含边缘情况） | 评估数据集 |
| **迭代优化** | 在评估集上测试 → 分析失败 → 优化 Prompt | Prompt vN.0 |
| **A/B 测试** | 线上小流量对比新旧 Prompt 效果 | A/B 测试报告 |
| **发布** | 全量上线 + 持续监控 | 线上监控面板 |

#### 评估流水线（Evaluation Pipeline）

| 维度 | 指标 | 测量方式 |
|------|------|---------|
| **准确性** | AI 输出与预期结果的匹配度 | 人工标注 + 自动评估（如 BLEU、F1） |
| **一致性** | 相同输入多次运行的结果稳定性 | 多次运行 → 计算结果方差 |
| **安全性** | 不产生有害/不当内容 | 安全分类器 + 人工审核 |
| **延迟** | 端到端响应时间 | P50/P95/P99 延迟监控 |
| **成本** | 每请求的 Token 消耗 | Token 用量统计 |

#### 技术风险

| 风险 | 概率 | 影响 | 缓解方案 |
|------|------|------|---------|
| AI API 服务宕机 | 中 | 极高 | 多模型 Provider fallback（如 OpenAI → Claude → 本地模型） |
| 模型升级导致质量回退 | 中 | 高 | 评估流水线 + 灰度发布 + 快速回滚 |
| Prompt Injection 攻击 | 中 | 高 | 输入过滤 + Output Guardrails + 权限隔离 |
| Token 成本超出预算 | 高 | 中 | 实时成本监控 + 告警 + 自动降级策略 |
| 数据隐私泄露 | 低 | 极高 | 不在 Prompt 中发送敏感数据 + 数据脱敏 + API 合规审查 |

---

### 行业基准

| KPI | 行业基准 | 优秀水平 | 合格水平 | 数据来源 |
|-----|---------|---------|---------|---------|
| **AI 任务成功率** | 80-90% | > 95% | > 85% | 内部评估集 |
| **Time to First Value (TTFV)** | 5-15 min | < 3 min | < 5 min | 用户行为数据 |
| **用户信任分** | — | NPS > 40 | NPS > 20 | NPS 调查 |
| **Human-in-the-loop 比例** | 30-50% | < 20% | < 40% | 产品使用数据 |
| **API 可用性** | 99.5% | > 99.9% | > 99.5% | 监控系统 |
| **P95 延迟** | 3-10s | < 3s | < 10s | APM 监控 |
| **每请求成本** | $0.01-0.10 | < $0.01 | < $0.05 | Token 用量统计 |
| **Day 7 留存率** | 20-35% | > 40% | > 25% | 用户行为数据 |
| **免费→付费转化** | 10-20% | > 25% | > 12% | 商业数据 |

**基准使用注意：**
- AI 任务成功率高度依赖任务类型：分类任务通常 > 90%，生成任务 80-85%
- TTFV 对 AI 产品尤其关键——如果用户 5 分钟内看不到价值，大概率流失
- Human-in-the-loop 比例随产品成熟度降低是正常的，但不应为 0%（意味着没有质量把控）
- 每请求成本差异巨大：简单 API 调用 < $0.01，复杂多步推理 > $0.10
- AI 产品的留存曲线通常比传统 SaaS 波动更大，需要更长的观察窗口

---

## AI 产品专项考量

### Prompt Engineering 原则

在 PRD 中评审 AI 交互设计时，检查以下原则是否被遵守：

| # | 原则 | 说明 | 检查方法 |
|---|------|------|---------|
| 1 | **任务边界清晰** | AI 的能力范围有明确定义 | PRD 中有"AI 能做 / 不能做"的明确列表 |
| 2 | **输入规范化** | 用户输入有格式引导和验证 | 有输入模板/示例，有输入长度/格式限制 |
| 3 | **输出结构化** | AI 输出有一致的格式 | 输出格式有 schema 定义，不是自由文本 |
| 4 | **错误可恢复** | AI 出错时用户有补救手段 | 有重试、编辑、回退、人工替代路径 |
| 5 | **反馈可收集** | 用户可以对 AI 结果打分/标注 | 有 thumbs up/down 或更细粒度的反馈机制 |

### 评估标准设计

PRD 中必须包含 AI 能力的评估标准：

```markdown
## AI 评估标准

### 功能性评估
- 评估集大小: ≥ 100 条测试用例
- 覆盖范围: 主流场景 70% + 边缘场景 20% + 对抗样本 10%
- 通过标准: 主流场景成功率 ≥ 90%，边缘场景 ≥ 70%，对抗样本 ≥ 50%

### 安全性评估
- 有害内容检测: 100% 拦截率
- 隐私泄露: 0 容忍
- Prompt Injection: 防御率 ≥ 95%

### 性能评估
- P50 延迟: < Xs
- P95 延迟: < Xs
- 每请求成本: < $X
```

### 人机协作模式

AI 产品的交互模式应该规划从低自主到高自主的渐进路径：

| 阶段 | 模式 | AI 角色 | 人的角色 | 适用场景 |
|------|------|---------|---------|---------|
| **L1: 辅助** | AI 提供建议，人做决策 | 顾问 | 决策者 | 信任建立初期、高风险场景 |
| **L2: 半自动** | AI 执行，人审核确认 | 执行者 | 审核者 | 信任中期、中等风险场景 |
| **L3: 自动+监督** | AI 自动执行，人抽查 | 自主执行者 | 监督者 | 信任成熟、低风险场景 |
| **L4: 全自动** | AI 完全自主，人只看结果 | 独立代理 | 观察者 | 充分信任、极低风险场景 |

**PRD 中需要标明：**
- MVP 阶段处于哪个自主级别
- 用户可以手动调整自主级别
- 每个级别的切换条件

### 成本优化策略

AI 产品的成本结构和传统 SaaS 完全不同，PRD 中必须包含成本预算：

| 层次 | 策略 | 实现难度 | 预期效果 |
|------|------|---------|---------|
| **Prompt 层** | 精简 Prompt、Few-shot → Zero-shot、指令压缩 | 低 | Token 降低 20-40% |
| **模型层** | 任务分级（简单用小模型）、Fine-tuning 专用模型 | 中 | 成本降低 50-70% |
| **缓存层** | 语义相似性缓存、结果缓存 | 中 | 成本降低 30-50%（高命中场景） |
| **架构层** | 批量处理、预计算、离线生成 | 高 | 成本降低 40-60%（适用于非实时场景） |

**成本预算模板：**

```markdown
## AI 成本预算

| 场景 | 月请求量 | 每请求 Token | 单价 | 月成本 |
|------|---------|-------------|------|--------|
| 核心功能 A | X 万次 | 平均 Y tokens | $Z/1M tokens | $W |
| 核心功能 B | X 万次 | 平均 Y tokens | $Z/1M tokens | $W |
| **总计** | | | | **$W** |

月收入预估: $X
AI 成本占比: Y%
目标: AI 成本占比 < 30%
```
