# Phase B: 成熟分析

```
═══════════════════════════════════════════════════════════
 Phase B: 成熟分析 — 功能确定后的深化
 思维模式: 精确→可测，每个字段都有定义
═══════════════════════════════════════════════════════════
```

### Phase 5: 用户旅程 + HITL 设计

> 读取参考: `{PLUGIN_ROOT}/references/hitl-design-patterns.md`

**Step 5.1: 端到端旅程**

画从首次触达到持续使用的完整旅程。每一步标注：涉及哪些实体、CRUD 操作。

**Step 5.2: HITL 级别标注**

每一步标注 HITL 级别（L1-L4）：
- L1: AI 生成，人审核
- L2: AI 建议，人选择
- L3: AI 自主，人监督
- L4: AI 自主执行

**Step 5.3: 渐进自主路径**

设计信任渐进路径：新用户 → 有首单 → 活跃用户 → 成熟用户，各阶段的 HITL 级别如何升级。

**Step 5.4: 失败路径设计**

设计用户不满意 AI 输出时的 3 种模式：重新生成、手动编辑、回退重来。

**Step 5.5: 关键时刻定义**

定义决定用户去留的 2-3 个触点（Moments of Truth）。

**Step 5.6: 输出**

写入 `{output_dir}/feature-spec.md` > `## Step 5: 用户旅程 + HITL`

**WAIT**

```
用户旅程 + HITL 设计完成。请审阅：

1. 端到端旅程是否覆盖了核心场景？
2. HITL 级别标注是否合理？有没有应该更自主/更保守的步骤？
3. 关键时刻（Moments of Truth）是否准确？
4. 失败路径是否足够？

请提供修改意见，或回复"继续"进入 AI 系统合约。
```

---

### Phase 6: AI 系统合约

> 读取参考: `{PLUGIN_ROOT}/references/ai-system-contracts.md`
> 读取参考: `{PLUGIN_ROOT}/templates/ai-product.md`

**Step 6.1: 识别 AI 交互点**

从用户旅程中提取所有 AI 交互点（每个需要 AI 处理的步骤 = 一个合约）。

**Step 6.2: 定义增强合约**

每个 AI 交互点定义完整合约（增强版），包含：

基础字段：
- 名称、触发、输入、处理、输出、质量标准、降级方案

增强字段：
- **评估规格**: 测量方法 + 评估数据集 + 上线阈值 + 持续监控
- **成本信封**: 单次 API 成本 x 月均调用 = 用户月成本
- **延迟预算**: 最大延迟 + 流式策略 + 进度指示
- **风险标记**: 可能出错 + 检测 + 缓解

**Step 6.3: 成本汇总**

计算单用户月 AI 成本 = 所有合约的成本之和。对照目标毛利率评估经济可行性。

**Step 6.4: 输出**

写入 `{output_dir}/feature-spec.md` > `## Step 6: AI 系统合约`

**WAIT**

```
AI 系统合约定义完成。请审阅：

1. AI 交互点是否完整？有没有遗漏的 AI 功能？
2. 成本信封是否可接受？总用户月成本 vs 订阅价格的毛利率是否 >=60%？
3. 延迟预算是否合理？
4. 风险标记是否完整？

请提供修改意见，或回复"继续"进入风险评估。
```

---

### Phase 7: 风险与伦理评估

> 继续使用: `{PLUGIN_ROOT}/references/feature-shaping.md`（风险评估部分）

> 如果 `--depth quick`，跳过此 Phase，直接进入 Phase 8。

**Step 7.1: 6 类 AI 风险评估**

评估 6 类 AI 特有风险：
- **质量风险**: AI 输出不靠谱
- **信任风险**: 用户按 AI 建议做但失败
- **经济风险**: API 成本超过收入
- **法律风险**: 生成内容侵权/违规
- **偏见风险**: 输出分布不均衡
- **依赖风险**: AI 供应商风险

每类评估：概率 + 影响 + 检测机制 + 缓解策略。

**Step 7.2: 红线对照**

对照 company.md 中的红线（如有），确认技术执行方案不触犯。

**Step 7.3: 输出**

写入 `{output_dir}/feature-spec.md` > `## Step 7: 风险评估`

**WAIT**

```
风险评估完成。请审阅：

1. 风险识别是否完整？
2. 缓解策略是否可行？
3. 有没有你认为更严重但未列出的风险？

请提供修改意见，或回复"继续"进入 Feature 塑形。
```

---

### Phase 8: Feature 塑形（Appetite-Constrained）

> 读取参考: `{PLUGIN_ROOT}/references/feature-shaping.md`（Cupcake + Appetite 部分）
> 读取参考: `{PLUGIN_ROOT}/references/prd-framework.md`（功能分级标准）

**Step 8.1: 定义 Cupcake**

定义 Cupcake 版本——最小端到端闭环，<= 6 Feature。

检查：
- 端到端可用（用户能从头走到尾）
- 价值可感知（用户走完后会说"这有用"）
- 首次使用 < 10 分钟

**Step 8.2: 完整 Feature 清单**

列出所有 Feature，每个包含：
- 对应 JTBD
- 所在页面
- 关联的 AI 合约
- HITL 级别
- 复杂度评估
- 兔子洞风险及应对
- 验收标准

**Step 8.3: 版本分组**

按版本分组（V1.0 / V1.1 / V1.2），V1.0 = Cupcake。

**Step 8.4: 输出**

写入 `{output_dir}/feature-spec.md` > `## Step 8: Feature 清单`

**WAIT**

```
Feature 塑形完成。请审阅：

1. Cupcake（V1.0）是否是最小但完整的闭环？Feature 是否 <= 6 个？
2. 完整 Feature 清单是否覆盖了所有 JTBD？
3. 版本分组是否合理？V1.0 是否足够小（<= 4 周开发量）？
4. 兔子洞识别是否准确？

请提供修改意见，或回复"继续"进入持续校准定义。
```

---

### Phase 9: 持续校准循环定义

> 继续使用: `{PLUGIN_ROOT}/references/feature-shaping.md`（持续校准部分）

**Step 9.1: 评估节奏**

定义 AI 质量评估节奏：
- 首月：每周跑评估
- 之后：双周跑评估
- 模型升级后：立即跑回归

**Step 9.2: 反馈→规格回流**

定义用户反馈如何回流到 AI 规格：
- 用户拒绝 AI 输出 → 收集 → 分析模式 → 更新 Prompt/质量标准

**Step 9.3: 涨标追踪**

定义"涨标"追踪方法——用户对 AI 质量的期望是否在上升（同质量 = 体验下降）。

**Step 9.4: 输出**

写入 `{output_dir}/feature-spec.md` > `## 附录: 持续校准`

更新 feature-spec.md header: `> 完成度: Step 1-8 已完成`

**WAIT**

```
持续校准循环定义完成。请审阅：

1. 评估节奏是否合理？
2. 反馈回流机制是否覆盖了关键场景？
3. 涨标追踪方法是否可量化？

请提供修改意见，或回复"继续"完成设计。
```

→ 跳转到 SKILL.md Phase 10（下一步引导）
