# 产品规划质量评估维度

> CPO 对产品规划（PRD / 版本规划）进行量化评估的 7 个核心维度。
> 每次 RLHF 评估时按此标准打分。

---

## 评分维度总览

| # | 维度 | 权重 | 核心关注点 |
|---|------|------|-----------|
| 1 | 原子化程度 | 20% | 功能是否拆到最小可交付单元 |
| 2 | MVP 精准度 | 18% | P0 是否是验证核心假设的最小集合 |
| 3 | 用户场景覆盖 | 15% | 用户故事是否覆盖核心 JTBD |
| 4 | 技术可行性 | 15% | 功能复杂度评估是否合理 |
| 5 | 竞品差异化 | 10% | 是否有明确的差异化切入点 |
| 6 | 指标可衡量性 | 12% | 成功指标是否可追踪、可验证 |
| 7 | 迭代逻辑性 | 10% | 版本间是否有清晰的验证→学习→迭代逻辑 |

**加权总分 = Σ(各维度得分 × 权重)**

---

## 维度 1: 原子化程度（权重 20%）

> 核心问题：功能是否拆分到可独立开发、独立测试、独立上线的最小单元？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 每个功能点都是独立可交付的原子单元，粒度精准，依赖关系清晰 | 功能点可以独立分配给一个开发者在 1-3 天内完成；依赖关系用 DAG 图表达；每个功能有独立的验收标准 |
| **7** | 大部分功能拆分合理，少量功能粒度偏大但可接受 | 80% 以上功能点可独立交付；个别功能需要 1 周以上但已标注原因；有基本的依赖说明 |
| **4** | 功能拆分粗糙，多个功能混在一起，难以独立交付 | 出现"用户系统""后台管理"等大块功能；功能间耦合严重；无法清晰分配开发任务 |
| **1** | 几乎没有拆分，按模块罗列而非按功能点拆分 | 只有"前端""后端""数据库"这样的模块划分；或只列了几个大功能模块，没有下钻 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **功能块过大** | "用户系统"包含注册+登录+权限+偏好 | 拆分到每个独立操作，逐个评估优先级 |
| **隐含耦合** | 功能 A 必须和功能 B 同时上线但未标注 | 画依赖关系图，标注硬依赖 vs 软依赖 |
| **粒度不一致** | 有些功能拆得很细，有些一笔带过 | 统一粒度标准：每个功能点 1-5 天工作量 |
| **缺少验收标准** | 功能描述模糊，无法判断"完成"的定义 | 每个功能点附带独立的验收标准 checklist |

---

## 维度 2: MVP 精准度（权重 18%）

> 核心问题：P0 功能是否是验证核心假设的最小集合？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | P0 是验证核心假设的最小集合，明确说明每个 P0 为什么是 P0 | P0 功能 ≤ 5 个；每个 P0 关联到一个核心假设；明确写出"如果删除这个功能，核心假设无法验证" |
| **7** | P0 集合基本合理，偶有不必要的功能混入 | P0 功能 ≤ 7 个；大部分 P0 有合理理由；有个别"锦上添花"的功能被标为 P0 |
| **4** | P0 过多或过少，没有明确的筛选标准 | P0 功能 > 7 个；或 P0 中混入了明显的 P1/P2；P0 选择缺乏逻辑支撑 |
| **1** | 没有优先级分级，或所有功能都标 P0 | 全部功能标 P0；没有 P0/P1/P2 分级；无法判断什么是 MVP 什么不是 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **P0 膨胀** | MVP 要做 3 个月，这不是 MVP | 用"如果只能做 3 个功能"测试裁剪 |
| **防御性 P0** | 把性能优化、管理后台标为 P0 | MVP 阶段可以用手动流程代替自动化 |
| **缺少假设关联** | P0 功能和核心假设没有对应关系 | 每个 P0 必须回答"它验证什么假设" |
| **忽略替代方案** | 没考虑更简单的实现方式 | 对每个 P0 问"有没有更简单的替代方案" |

---

## 维度 3: 用户场景覆盖（权重 15%）

> 核心问题：用户故事是否覆盖了核心 JTBD（Jobs to be Done）？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 用户故事覆盖所有核心 JTBD，角色具体、场景真实、有验收标准 | 用户角色具体到职业+场景+痛点；每个故事有清晰的"角色-行动-收益"格式；覆盖主流程+关键分支流程；有边界情况考虑 |
| **7** | 覆盖了主要 JTBD，用户故事基本规范 | 核心用户路径有故事覆盖；角色描述较具体；缺少部分边界场景或次要角色 |
| **4** | 用户故事不完整或不规范，缺少关键场景 | 用户角色泛化（"用户"而非具体角色）；缺少完整的故事格式；核心路径有盲区 |
| **1** | 没有用户故事，直接列功能 | 只有功能列表没有用户视角；或只有一句话需求描述；完全看不出为谁做、为什么做 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **角色泛化** | "作为一个用户" | 具体到"作为月 GMV 10-50 万的跨境电商运营" |
| **收益不可衡量** | "以便提高效率" | 改为"以便将单个 SKU 上架时间从 30 分钟缩短到 5 分钟" |
| **缺少验收标准** | 只有故事没有 AC | 每个故事附带 3-5 条独立可验证的验收标准 |
| **只覆盖 happy path** | 没有错误场景、边界场景 | 补充"当 XX 失败时"的故事 |
| **缺少 JTBD 映射** | 故事和用户真实需求脱节 | 先做 JTBD 分析，再写用户故事 |

---

## 维度 4: 技术可行性（权重 15%）

> 核心问题：功能的复杂度评估是否合理？技术风险是否被识别？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 每个功能有合理的复杂度评估，技术风险已识别并有应对方案 | 使用 5 维度复杂度矩阵评估；技术依赖项全部列出；高风险功能有 POC 计划；有技术债务预算 |
| **7** | 复杂度评估基本准确，主要技术风险已识别 | 有复杂度分级（简单/中等/复杂）；核心功能的技术方案有初步思路；关键依赖项已列出 |
| **4** | 复杂度评估粗糙或偏乐观，部分技术风险被忽略 | 所有功能标"简单"或"中等"；依赖第三方服务但没有 fallback；缺少性能需求量化 |
| **1** | 没有技术可行性评估，或评估明显不合理 | 没有任何技术方案描述；时间估算严重偏离现实；忽略明显的技术瓶颈 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **乐观偏差** | 每个功能都估 2 天 | 使用 5 维度评估矩阵，按总分分级 |
| **忽略依赖** | 没列第三方 API 依赖 | 单独列出所有外部依赖和对应风险 |
| **无 fallback** | 依赖单一技术方案 | 对高风险技术点准备 Plan B |
| **性能留白** | "要快"但没量化 | 量化到具体指标：响应时间 < Xms，QPS > Y |

---

## 维度 5: 竞品差异化（权重 10%）

> 核心问题：是否有明确的差异化切入点？而不是做一个"什么都有但什么都不突出"的产品？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 差异化策略清晰，有具体的竞品对比矩阵和独特价值主张 | 竞品对比矩阵覆盖 3-5 个主要竞品；差异化点有数据支撑；价值主张可以一句话说清 |
| **7** | 有差异化思考，但深度不够 | 列出了竞品和差异化方向；但缺少数据支撑或用户验证；差异化点可能不是用户真正在意的 |
| **4** | 差异化模糊，或只是"我们也有"的竞品对标 | 竞品分析只是罗列功能；没有明确"我们不一样在哪"；或差异化点是"更好的 UI" |
| **1** | 没有竞品分析，或完全忽略竞争环境 | 没有提到任何竞品；或只说"市场很大机会很多"；不知道为什么用户会选择我们 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **伪差异化** | "我们 UI 更好" | 差异化要在功能/模式/定价层面，不是装修层面 |
| **无取舍** | 每个竞品功能都要做 | 明确写出"我们不做什么"以及"为什么" |
| **缺少用户验证** | 差异化点是团队脑补的 | 用 5-10 个目标用户验证差异化假设 |
| **忽略进入壁垒** | 差异化容易被抄袭 | 评估差异化的可持续性和防御性 |

---

## 维度 6: 指标可衡量性（权重 12%）

> 核心问题：成功指标是否可追踪、可验证？上线后能不能知道做对了还是做错了？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 每个功能有可衡量的成功指标，有基线值、目标值和测量方式 | North Star Metric 明确；每个 P0 功能有 1-2 个关键指标；指标有基线值和目标值；数据采集方案明确 |
| **7** | 有成功指标但不够完善 | 有核心指标定义；缺少部分基线值或测量方式；数据采集方案不完整 |
| **4** | 指标模糊或不可测量 | 指标是"提升用户体验""增加收入"等模糊描述；没有基线和目标值；不知道用什么工具测量 |
| **1** | 没有成功指标定义 | PRD 中完全没有指标部分；或只有"上线即成功"这样的伪指标 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **模糊指标** | "提升用户满意度" | 量化："NPS 从 30 提升到 45" |
| **无基线** | "转化率提升 20%" — 从什么基线？ | 标注当前基线值，或标注"待首版数据确认" |
| **不可追踪** | 指标定义了但没有采集方案 | 明确用什么工具、在什么节点采集什么数据 |
| **虚荣指标** | 只看 PV、注册量 | 增加留存、激活率、付费转化等实质性指标 |
| **短视指标** | 只有上线第一周的指标 | 分层：短期（1 周）、中期（1 月）、长期（1 季度） |

---

## 维度 7: 迭代逻辑性（权重 10%）

> 核心问题：版本间是否有清晰的"验证 → 学习 → 迭代"逻辑？

### 评分标准

| 分数 | 标准 | 典型表现 |
|------|------|---------|
| **10** | 版本间有严密的逻辑链，每个版本的规划基于上个版本的学习 | V1.0 → 验证核心假设 → V1.1 基于 V1.0 数据决策 → V1.2 基于 V1.1 学习优化；每个版本目标明确，范围可控 |
| **7** | 有基本的迭代逻辑，但不够严密 | 版本划分合理但缺少"学习"环节；V1.1 的规划不完全基于 V1.0 的数据反馈 |
| **4** | 版本规划是"功能堆叠"而非"假设验证" | V1.1 只是 V1.0 没做完的功能继续做；没有数据反馈环节；版本之间缺乏逻辑关联 |
| **1** | 没有版本规划或只有一个大版本 | 没有迭代计划；或者"先做完再说"；不知道上线后要观察什么、调整什么 |

### 常见扣分模式

| 扣分项 | 典型症状 | 改进建议 |
|--------|---------|---------|
| **功能堆叠** | V1.1 = V1.0 + 剩下的功能 | 每个版本有独立的假设和验证目标 |
| **无学习环节** | 版本间没有数据回顾 | 每个版本规划前置"上个版本的核心发现" |
| **版本过大** | 每个版本都是 2-3 个月 | 控制每个版本 1 个月以内 |
| **不可调整** | 规划完就执行，没有调整空间 | 每个版本预留 20% 弹性空间 |

---

## 一票否决规则（Veto Rules）

以下任一条件触发，无论总分多高，一律判定"重做"：

| # | 否决条件 | 判定标准 | 原因 |
|---|---------|---------|------|
| **V1** | 原子化程度 < 4 | 功能粒度过大，无法执行 | 功能块太大，开发无法拆分任务、无法评估工期、无法独立交付 |
| **V2** | MVP 精准度 < 3 | MVP 过重，不是 MVP | P0 功能过多或没有优先级分级，等于没有裁剪等于没有规划 |
| **V3** | 无用户故事直接列功能 | 用户场景覆盖 = 1 | 缺乏用户视角意味着在闭门造车，功能可能全都不是用户需要的 |
| **V4** | 所有功能标 P0 | MVP 精准度 = 1 | 没有优先级等于没有规划，无法做出任何取舍决策 |

**否决后操作：**
1. 指出触发了哪条否决规则
2. 给出具体的改进方向（不是泛泛的"需要改进"）
3. 提供改进示例（用当前 PRD 的内容作为素材）
4. 不需要重新评估其他维度，直接回到 Phase 1 重做

---

## 动态权重调整

不同产品类型的评估重点不同，权重需要根据产品类型调整：

### SaaS 产品

| 维度 | 默认权重 | SaaS 权重 | 调整原因 |
|------|---------|----------|---------|
| 原子化程度 | 20% | **22%** | SaaS 功能多且复杂，原子化拆分更关键 |
| MVP 精准度 | 18% | **20%** | SaaS 最忌 MVP 过重，需要快速验证 |
| 用户场景覆盖 | 15% | 15% | 保持不变 |
| 技术可行性 | 15% | **16%** | SaaS 技术栈复杂，可行性评估更重要 |
| 竞品差异化 | 10% | 10% | 保持不变 |
| 指标可衡量性 | 12% | **10%** | SaaS 指标体系相对成熟，权重可略降 |
| 迭代逻辑性 | 10% | **7%** | SaaS 迭代模式较固定 |

### 电商产品

| 维度 | 默认权重 | 电商权重 | 调整原因 |
|------|---------|---------|---------|
| 原子化程度 | 20% | 18% | 电商交易流程耦合度高，原子化空间有限 |
| MVP 精准度 | 18% | 15% | 电商 MVP 通常需要完整交易闭环 |
| 用户场景覆盖 | 15% | **20%** | 电商用户路径复杂（买家+卖家），场景覆盖更关键 |
| 技术可行性 | 15% | **17%** | 支付、库存等技术要求高 |
| 竞品差异化 | 10% | **12%** | 电商竞争激烈，差异化更重要 |
| 指标可衡量性 | 12% | **10%** | 电商指标（GMV、转化率）比较标准 |
| 迭代逻辑性 | 10% | **8%** | 保持不变 |

### 内容产品

| 维度 | 默认权重 | 内容权重 | 调整原因 |
|------|---------|---------|---------|
| 原子化程度 | 20% | 15% | 内容产品功能相对简单 |
| MVP 精准度 | 18% | 15% | 内容产品 MVP 通常较轻 |
| 用户场景覆盖 | 15% | **20%** | 内容消费场景多样，覆盖很关键 |
| 技术可行性 | 15% | 10% | 技术复杂度通常较低 |
| 竞品差异化 | 10% | **18%** | 内容产品高度同质化，差异化决定生死 |
| 指标可衡量性 | 12% | **12%** | 保持不变 |
| 迭代逻辑性 | 10% | **10%** | 保持不变 |

### AI 产品

| 维度 | 默认权重 | AI 权重 | 调整原因 |
|------|---------|--------|---------|
| 原子化程度 | 20% | 18% | AI 模块间耦合度中等 |
| MVP 精准度 | 18% | **20%** | AI 产品最忌过度承诺能力范围 |
| 用户场景覆盖 | 15% | 15% | 保持不变 |
| 技术可行性 | 15% | **20%** | AI 技术不确定性高，可行性评估最关键 |
| 竞品差异化 | 10% | 8% | AI 产品差异化更多在技术层，已反映在技术可行性中 |
| 指标可衡量性 | 12% | **12%** | AI 效果评估尤其需要量化 |
| 迭代逻辑性 | 10% | **7%** | AI 产品迭代更依赖模型优化而非功能堆叠 |

---

## 评分流程

### Step 1: 确定产品类型
根据产品类型选择对应的权重表。如果是新品类或混合类型，使用默认权重。

### Step 2: 逐维度评分
对每个维度独立打分（1-10 分），参考上述 10/7/4/1 标准。分数可以取中间值（如 8.5）。

### Step 3: 检查一票否决
检查 4 条 Veto Rules。任一触发则直接判定"重做"，不需要继续评分。

### Step 4: 计算加权总分
```
总分 = Σ(维度得分 × 该产品类型对应权重)
```

### Step 5: 生成评估报告
按 `rlhf-loop.md` 中的评估报告模板输出完整评估结果。

### Step 6: 做出决策
根据决策分层（≥8.0 通过 / 7.0-7.9 小幅修订 / 6.0-6.9 大幅修订 / <6.0 重做）确定后续行动。
